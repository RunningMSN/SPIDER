import uuid
import os
import shutil
import subprocess
import math
from helpers.settings import blast_columns_fmt_6
import pandas as pd
import numpy as np

def crawl(fasta, db_loc, slide_limit, length, identity, primer_size):
    """
    Runs SPIDER to identify VFs in the supplied fasta file.
    """
    print(f"Beginning to crawl {fasta} using the following settings:")
    print(f"Primer Size: {primer_size}bp")
    print(f"Slide Limit: {slide_limit}%")
    print(f"Length: {length}%")
    print(f"Identity: {identity}%")

    # Create a temporary directory
    temp_directory = f"spider_tmp_{uuid.uuid4().hex}"

    # Setup crawler environment
    setup(fasta, temp_directory)

    # Iterate through all VFs to test
    with open(db_loc, "r") as database:
        # Load VFs by header and sequence
        for header, sequence in zip(database, database):
            print(f"Testing {header}")
            extract_vf(header, sequence.strip(), slide_limit, primer_size, temp_directory)
            # break # TEMPORARY BREAK TODO: REMOVE THIS

    # Cleanup temporary environment
    cleanup(temp_directory)

def setup(fasta, temp_directory):
    # Create temporary directory
    os.makedirs(temp_directory)

    # Copy assembly to the temp directory
    shutil.copy(fasta, f"{temp_directory}/reference.fasta")

    # Make blast DB for primer lookup
    makeblastdb_cmd = ["makeblastdb", "-in", f"{temp_directory}/reference.fasta", 
                       "-dbtype", "nucl"]
    subprocess.run(makeblastdb_cmd)

def cleanup(temp_directory):
    """
    Cleans up the temporary files generated by this script.
    """
    # Remove temporary directory
    shutil.rmtree(temp_directory)


def extract_vf(header, sequence, slide_limit, primer_size, temp_directory):
    """
    Identifies the virulence factor if present.
    """
    # Make directory for the VF
    vf_directory = f"{temp_directory}/{header.split(' ')[0].replace('>','')}"
    os.makedirs(vf_directory)

    # Find sequence length for number of primers to generate
    primer_slide_limit_nt = math.floor(slide_limit / 100 * len(sequence))

    # Number of primers to generate is primer size subtracted from the limit
    number_primers = primer_slide_limit_nt - primer_size

    # Generate the forward primers
    with open(f"{vf_directory}/forward_primers.fasta", "w") as forward_primers:
        for i in range(0, number_primers):
            forward_primers.write(f">forward_{i}\n{sequence[i:i+primer_size]}\n")

    # Generate the reverse primers
    with open(f"{vf_directory}/reverse_primers.fasta", "w") as reverse_primers:
        for i in range(0, number_primers):
            reverse_primers.write(f">reverse_{i}\n{sequence[len(sequence)-i-primer_size:len(sequence)-i]}\n")

    # BLAST both sets of primers
    for primer_set in ("forward_primers", "reverse_primers"):
        blast_cmd = ["blastn", "-query", f"{vf_directory}/{primer_set}.fasta", 
                     "-db", f"{temp_directory}/reference.fasta", 
                     "-outfmt", "6", "-word_size", f"{primer_size}", 
                     "-out", f"{vf_directory}/{primer_set}.blast.txt"]
        subprocess.run(blast_cmd)

    parse_primer_matches(vf_directory, len(sequence))


def parse_primer_matches(vf_directory, expected_vf_length):
    # Parse the best forward primer match(es)
    forward_matches = pd.read_csv(f"{vf_directory}/forward_primers.blast.txt", sep="\t", header=None, names=blast_columns_fmt_6)
    if len(forward_matches) > 0:
        forward_matches["qseqid"] = forward_matches["qseqid"].str.replace("forward_", "")
        # Sort to make sure first primers are kept
        forward_matches["qseqid"] = forward_matches["qseqid"].astype(int)
        forward_matches.sort_values(by="qseqid", ascending = True, inplace= True)
        # Keep only matches for the best primer
        forward_matches = forward_matches[forward_matches["qseqid"] == forward_matches["qseqid"][0]]
        # Add information about strand
        forward_matches["strand"] = np.where(forward_matches["sstart"] < forward_matches["send"], "+", "-")

    # If no matches found, set to null
    else:
        forward_matches = None
    
    # Parse the best reverse primer match(es)
    reverse_matches = pd.read_csv(f"{vf_directory}/reverse_primers.blast.txt", sep="\t", header=None, names=blast_columns_fmt_6)
    if len(reverse_matches) > 0:
        reverse_matches["qseqid"] = reverse_matches["qseqid"].str.replace("reverse_", "")
        # Sort to make sure first primers are kept
        reverse_matches["qseqid"] = reverse_matches["qseqid"].astype(int)
        reverse_matches.sort_values(by="qseqid", ascending = True, inplace= True)
        # Keep only matches for the best primer
        reverse_matches = reverse_matches[reverse_matches["qseqid"] == reverse_matches["qseqid"][0]]
        # Add information about strand
        reverse_matches["strand"] = np.where(reverse_matches["sstart"] < reverse_matches["send"], "+", "-")

    # If no matches found, set to null
    else:
        reverse_matches = None

    primer_pairs, errors = sort_primer_pairs(forward_matches, reverse_matches, expected_vf_length)

    # TODO: Add validation for the primer pairs

    # TODO: Add extraction of the VF sequence


def sort_primer_pairs(forward_matches, reverse_matches, expected_vf_length):
    """
    Identifies primer pairs. The total number of pairs will be whichever 
    direction primer had less hits. E.g. if forward primer was found once, but
    reverse was found twice, one pair of primers will be calculated.

    No validation of these primer matches is performed here. Pairs will be 
    validated in a separate function before finalizing VF call.

    Returns:
    primer_pairs_indices: List of tuples containing indices of the 
    forward and reverse primers that form pairs
    errors: List of errors that describes why VF failed to be identified
    """
    # Store pairs as tuples of forward and reverse index
    primer_pairs_indices = []
    errors = []
    
    # Identify primer pairings where smallest number of primers in one set are paired with primers from other set
    if forward_matches is not None and reverse_matches is not None: 
        # Set positions to integers
        forward_matches["sstart"] = forward_matches["sstart"].astype(int)
        forward_matches["send"] = forward_matches["send"].astype(int)
        reverse_matches["sstart"] = reverse_matches["sstart"].astype(int)
        reverse_matches["send"] = reverse_matches["send"].astype(int)

        # Store indices before getting merged
        forward_matches["index"] = forward_matches.index
        reverse_matches["index"] = reverse_matches.index 
        
        # Merge on sseqid and strand to make sure primers are on correct contig and in correct direction
        pairs = pd.merge(forward_matches, reverse_matches, on=["sseqid", "strand"], suffixes=("_f", "_r"))

        # Calculate distance from expected length to get primer pairs distances
        if len(pairs) > 0:
            # Calculate distances
            pairs["distance"] = abs(abs(pairs["sstart_f"] - pairs["send_r"]) - expected_vf_length)
            
            # Sort by distance
            pairs.sort_values(by="distance", ascending = True, inplace= True)

            # Number of primer pairs to target is the lesser of number of primers identified in forward or reverse direction
            target_pairs_count = min(len(forward_matches), len(reverse_matches))
            used_forward = set()
            used_reverse = set()
            # Number of pairs identified so far
            pairs_count = 0
            
            # Iterate throw each pair by ascending distance
            for index, row in pairs.iterrows():
                if pairs_count < target_pairs_count:
                    # If current smallest distance, and have not used this primer pair, then add it in
                    if not row["index_f"] in used_forward and not row["index_r"] in used_reverse:
                        used_forward.add(row["index_f"])
                        used_reverse.add(row["index_r"])
                        primer_pairs_indices.append((row["index_f"],row["index_r"]))
                # If have satisfied target number of primer pairs, then break from loop
                else:
                    break
    elif forward_matches is None and reverse_matches is None:
        errors.append("Both forward and reverse primers were not identified")
    elif forward_matches is None:
        errors.append(f"The forward primer was not identified, a reverse primer was found.") # TODO: Add slide amount that identified the found primer
    elif reverse_matches is None:
        errors.append(f"The reverse primer was not identified, a forward primer was found.") # TODO: Add slide amount that identified the found primer
    return primer_pairs_indices, errors