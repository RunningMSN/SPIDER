import uuid
import os
import shutil
import subprocess
import math
from helpers.settings import BLAST_COLUMNS_FMT_6, SPIDER_RESULTS_COLUMNS
import pandas as pd
import numpy as np
from pyfaidx import Fasta
from Bio.Align import PairwiseAligner
from Bio.Seq import Seq
from itertools import combinations

def crawl(fasta, db_loc, slide_limit, length_limit, identity_limit, primer_size):
    """
    Runs SPIDER to identify VFs in the supplied fasta file.

    Arguments:
        fasta -- Location of assembly to query
        db_loc -- Location of target datavase
        slide_limit -- Percentage of target gene that SPIDER can slide
        length_limit -- Percentage limit of length for which a VF will validate
        identity_limit -- Threshold identity at which to call a VF as present
        primer_size -- Size of primer for in-silico PCR

    Returns:
        df_results -- Results of crawler in the form of pandas dataframe
    """
    # Create a temporary directory name
    temp_directory = f"spider_tmp_{uuid.uuid4().hex}"

    # Setup crawler environment and temp directory
    setup(fasta, temp_directory)

    # Iterate through all VFs to test
    all_results = []
    with open(db_loc, "r") as database:
        # Load VFs by header and sequence
        for header, sequence in zip(database, database):
            results = identify_vf(header, sequence.strip(), slide_limit, primer_size, temp_directory, length_limit, identity_limit)
            for result in results:
                # Add header to the result as first item
                result = (fasta,header.strip().replace(">",""),) + result
                # Append to overall results
                all_results.append(result)
    spider_results = pd.DataFrame(all_results, columns=SPIDER_RESULTS_COLUMNS)

    # Add warnings for overlaps
    spider_results = find_overlaps(spider_results)

    # Cleanup temporary environment
    cleanup(temp_directory)

    # Return results
    return spider_results

def setup(fasta, temp_directory):
    """
    Sets up a working environment for SPIDER.

    Arguments:
        fasta -- Location of the assembly being searched
        temp_directory -- Location of temporary directory to be made
    """
    # Create temporary directory
    os.makedirs(temp_directory)

    # Copy assembly to the temp directory
    shutil.copy(fasta, f"{temp_directory}/reference.fasta")

    # Make blast DB for primer lookup
    makeblastdb_cmd = ["makeblastdb", "-in", f"{temp_directory}/reference.fasta", 
                       "-dbtype", "nucl"]
    subprocess.run(makeblastdb_cmd, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)


def cleanup(temp_directory):
    """
    Cleans up the temporary files generated by this script.

    Arguments:
        temp_directory -- Temporary SPIDER working directory that 
                          will be removed.
    """
    # Remove temporary directory
    shutil.rmtree(temp_directory)


def identify_vf(header, ref_sequence, slide_limit, primer_size, temp_directory, length_limit, identity_limit):
    """
    Identifies the virulence factor if present.

    Arguments:
        header -- VF header
        ref_sequence -- VF reference sequence
        slide_limit -- User set slide limit for primers
        primer_size -- User provided primer length
        temp_directory -- Temporary directory to use
        length_limit -- User provided limit on length to use
        identity_limit -- User provided identity limit to use

    Returns:
        results -- List of tuples that contain results. Each tuple is in the format: 
                   (Valid, Contig, Start, F_Slide, End, R_Slide, Strand, Identity, VF_length, 
                   Ref_Length, Coverage_Perc_Len, Coverage_Perc_Align, Message)
    """
    # Make directory for the VF
    vf_directory = f"{temp_directory}/{header.split(' ')[0].replace('>','')}"
    os.makedirs(vf_directory)

    # Find sequence length for number of primers to generate
    ref_length = len(ref_sequence)
    number_primers = math.floor(slide_limit / 100 * ref_length)
    # Make sure that the number of primers can never be 0
    if number_primers < 1: number_primers = 1

    # Generate the forward primers
    with open(f"{vf_directory}/forward_primers.fasta", "w") as forward_primers:
        for i in range(0, number_primers):
            forward_primers.write(f">forward_{i}\n{ref_sequence[i:i+primer_size]}\n")

    # Generate the reverse primers
    with open(f"{vf_directory}/reverse_primers.fasta", "w") as reverse_primers:
        for i in range(0, number_primers):
            reverse_primers.write(f">reverse_{i}\n{ref_sequence[ref_length-i-primer_size:ref_length-i]}\n")

    # BLAST both sets of primers
    for primer_set in ("forward_primers", "reverse_primers"):
        blast_cmd = ["blastn", "-query", f"{vf_directory}/{primer_set}.fasta", 
                     "-db", f"{temp_directory}/reference.fasta", 
                     "-outfmt", "6", "-word_size", f"{primer_size}", 
                     "-out", f"{vf_directory}/{primer_set}.blast.txt"]
        subprocess.run(blast_cmd)
        
    # Obtain primer matches
    forward_matches, reverse_matches = parse_primer_matches(vf_directory)
    # Sort the primers into pairs
    primer_pairs, error = sort_primer_pairs(forward_matches, reverse_matches, ref_length)
    # Store returned output
    results = []

    # Extract VF sequence for each primer pair
    if len(primer_pairs) > 0:
        vf_extracted_counter = 0
        for pair in primer_pairs:
            contig, start, end, strand, forward_slide, reverse_slide = extract_vf_location(pair, forward_matches, reverse_matches)
            
            # Extract the VF sequence
            vf_sequence, vf_length = extract_vf_sequence(contig, start, end, temp_directory)
            
            # Align the VF to get identity and coverage
            identity, coverage_percent_length, coverage_alignment = align_vf(ref_sequence, vf_sequence, strand)
            
            # Check validity of VF
            valid, error = validate_vf(identity, coverage_percent_length, length_limit, identity_limit)

            # Add tuple for output: (Valid, Start, F_Slide, End, R_Slide, Strand, Identity, VF_length, Ref_Length, Coverage_Perc_Len, Coverage_Perc_Align, Error Message)
            results.append((valid, contig, start, forward_slide, end, reverse_slide, strand, identity, vf_length, ref_length, coverage_percent_length, coverage_alignment, error))
    else:
        results.append((False, "NA", "NA", "NA", "NA", "NA", "NA", "NA", "NA", ref_length, "NA", "NA", error))

    return results

def parse_primer_matches(vf_directory):
    """
    Identifies the best primer match for VF.

    Arguments:
        vf_directory -- Temporary directory being used for the VF
    
    Returns:
        forward_matches - Pandas dataframe with best forward primer matches
        reverse_matches - Pandas dataframe with best reverse primer matches
    """
    # Parse the best forward primer match(es)
    forward_matches = pd.read_csv(f"{vf_directory}/forward_primers.blast.txt", sep="\t", header=None, names=BLAST_COLUMNS_FMT_6)
    if len(forward_matches) > 0:
        # Set names to just the slide amount
        forward_matches["qseqid"] = forward_matches["qseqid"].str.replace("forward_", "")
        # Sort to make sure first primers are kept
        forward_matches["qseqid"] = forward_matches["qseqid"].astype(int)
        forward_matches.sort_values(by="qseqid", ascending = True, inplace= True)
        # Keep only matches for the best primer
        forward_matches = forward_matches[forward_matches["qseqid"] == forward_matches["qseqid"][0]]
        # Add information about strand
        forward_matches["strand"] = np.where(forward_matches["sstart"] < forward_matches["send"], "+", "-")

    # If no matches found, set to null
    else:
        forward_matches = None
    
    # Parse the best reverse primer match(es)
    reverse_matches = pd.read_csv(f"{vf_directory}/reverse_primers.blast.txt", sep="\t", header=None, names=BLAST_COLUMNS_FMT_6)
    if len(reverse_matches) > 0:
        # Set names to just the slide amount
        reverse_matches["qseqid"] = reverse_matches["qseqid"].str.replace("reverse_", "")
        # Sort to make sure first primers are kept
        reverse_matches["qseqid"] = reverse_matches["qseqid"].astype(int)
        reverse_matches.sort_values(by="qseqid", ascending = True, inplace= True)
        # Keep only matches for the best primer
        reverse_matches = reverse_matches[reverse_matches["qseqid"] == reverse_matches["qseqid"][0]]
        # Add information about strand
        reverse_matches["strand"] = np.where(reverse_matches["sstart"] < reverse_matches["send"], "+", "-")

    # If no matches found, set to null
    else:
        reverse_matches = None

    return forward_matches, reverse_matches


def sort_primer_pairs(forward_matches, reverse_matches, expected_vf_length):
    """
    Identifies primer pairs. The total number of pairs will be whichever 
    direction primer had less hits. E.g. if forward primer was found once, but
    reverse was found twice, one pair of primers will be calculated.

    No validation of these primer matches is performed here. Pairs will be 
    validated in a separate function before finalizing VF call.

    Arguments:
        forward_matches -- Pandas dataframe containing forward primer matches
        reverse_matches -- Pandas dataframe containing the reverse primer matches
        expected_vf_length -- Expected length of the VF is the length of the reference
                              VF sequence.

    Returns:
        primer_pairs_indices -- List of tuples containing indices of the 
        forward and reverse primers that form pairs
        error -- Reason why VF failed to be identified
    """
    # Store pairs as tuples of forward and reverse index
    primer_pairs_indices = []
    # Store error message
    error = ""

    # Identify primer pairings where smallest number of primers in one set are paired with primers from other set
    if forward_matches is not None and reverse_matches is not None: 
        # Set positions to integers
        forward_matches["sstart"] = forward_matches["sstart"].astype(int)
        forward_matches["send"] = forward_matches["send"].astype(int)
        reverse_matches["sstart"] = reverse_matches["sstart"].astype(int)
        reverse_matches["send"] = reverse_matches["send"].astype(int)

        # Store indices before getting merged
        forward_matches["index"] = forward_matches.index
        reverse_matches["index"] = reverse_matches.index 
        
        # Merge on sseqid and strand to make sure primers are on correct contig and in correct direction
        pairs = pd.merge(forward_matches, reverse_matches, on=["sseqid", "strand"], suffixes=("_f", "_r"))

        if len(pairs) == 0:
            error = f"Forward primers found on {','.join(pd.unique(forward_matches['sseqid'].astype(str)))} ({'/'.join(forward_matches['strand'])}) and reverse primers found on {','.join(pd.unique(reverse_matches['sseqid'].astype(str)))} ({'/'.join(reverse_matches['strand'])}) "

        # Filter out bad pairs that are in improper order
        valid_ordered_pairs = pairs[
            ((pairs["strand"] == "+") & (pairs["sstart_f"] < pairs["send_r"])) |
            ((pairs["strand"] == "-") & (pairs["send_r"] < pairs["sstart_f"]))
        ].copy()

        # Calculate distance from expected length to get primer pairs distances
        if len(valid_ordered_pairs) > 0:
            # Calculate distances
            valid_ordered_pairs["distance"] = abs(abs(valid_ordered_pairs["sstart_f"] - valid_ordered_pairs["send_r"]) - expected_vf_length)
            
            # Sort by distance
            valid_ordered_pairs.sort_values(by="distance", ascending = True, inplace= True)

            # Number of primer pairs to target is the lesser of number of primers identified in forward or reverse direction
            target_pairs_count = min(len(forward_matches), len(reverse_matches))
            used_forward = set()
            used_reverse = set()
            # Number of pairs identified so far
            pairs_count = 0
            
            # Iterate throw each pair by ascending distance
            for index, row in valid_ordered_pairs.iterrows():
                if pairs_count < target_pairs_count:
                    # If current smallest distance, and have not used this primer pair
                    if not row["index_f"] in used_forward and not row["index_r"] in used_reverse and row["distance"] > 0:
                        used_forward.add(row["index_f"])
                        used_reverse.add(row["index_r"])
                        primer_pairs_indices.append((row["index_f"],row["index_r"]))
                # If have satisfied target number of primer pairs, then break from loop
                else:
                    break
            # Error is empty
            error = ""
        elif len(pairs) > 0:
            error = "Forward and reverse primers were identified, but they were not in the correct order (i.e. F after R or R after F)."
    elif forward_matches is None and reverse_matches is None:
        error = "Neither forward nor reverse primers were not identified."
    elif forward_matches is None:
        error = f"The forward primer was not identified, a reverse primer was found with slide of {reverse_matches["qseqid"][0]}."
    elif reverse_matches is None:
        error = f"The reverse primer was not identified, a forward primer was found with slide of {forward_matches["qseqid"][0]}."

    return primer_pairs_indices, error


def extract_vf_location(primer_pair_indices, forward_matches, reverse_matches):
    """
    Returns the location of the virulence factor given a set of primer pair indices
    for the forward and reverse BLAST searches.

    Arguments:
        primer_pair_indiced -- Tuple of indices for the forward and reverse
                               BLAST matches for the primers.
        forward_matches -- Pandas dataframe containing forward primer matches
        reverse_matches -- Pandas dataframe containing the reverse primer matches

    Return:
        contig -- Contig on which VF is located
        start -- Start position
        end -- End position
        strand -- +/- strand
        forward_slide -- # of bases slide on forward primer
        reverse_slide -- # of bases slide on reverse primer
    """
    # Obtain location of VF
    contig = forward_matches.iloc[primer_pair_indices[0]]["sseqid"]
    strand = forward_matches.iloc[primer_pair_indices[0]]["strand"]

    
    ## If positive strand then going from sstart to send
    if strand == "+":
        # Start position is where primer landed and subtract off primer slide amount
        start = int(forward_matches.iloc[primer_pair_indices[0]]["sstart"]) - int(forward_matches.iloc[primer_pair_indices[0]]["qseqid"])
        # End position is where primer landed and add primer slide amount
        end = int(reverse_matches.iloc[primer_pair_indices[1]]["send"]) + int(reverse_matches.iloc[primer_pair_indices[1]]["qseqid"])
    # If negative strand, then reverse the direction
    else:
        # Start position is where reverse primer landed and add slide amount
        start = int(reverse_matches.iloc[primer_pair_indices[1]]["send"]) - int(reverse_matches.iloc[primer_pair_indices[1]]["qseqid"])
        # End position is where forward primer landed, and subtract slide amount
        end = int(forward_matches.iloc[primer_pair_indices[0]]["sstart"]) + int(forward_matches.iloc[primer_pair_indices[0]]["qseqid"])
    
    forward_slide = forward_matches.iloc[primer_pair_indices[0]]["qseqid"]
    reverse_slide = reverse_matches.iloc[primer_pair_indices[1]]["qseqid"]

    return contig, start, end, strand, forward_slide, reverse_slide


def extract_vf_sequence(contig, start, end, temp_directory):
    """
    Extracts the virulence factor sequence using pyfaidx.

    Arguments:
        contig -- Contig on which VF is located.
        start -- Start position
        end -- End position
        temp_directory -- Working directory for SPIDER

    Returns:
        seq -- Virulence factor sequence that was identified
        length -- Length of the virulence factor sequence extracted
    """
    genome = Fasta(f"{temp_directory}/reference.fasta")
    contig = str(contig)
    # Must subtract 1 base from start since python index at 0 and BLAST coordinate index at 1
    seq = str(genome[contig][start-1:end])
    length = end-start+1 # Add 1 to be inclusive of ends
    
    return seq, length


def align_vf(reference_sequence, vf_sequence, vf_strand):
    """
    Aligns VF sequence to the reference sequence.

    Arguments:
        reference_sequence -- Target virulence factor sequence
        vf_sequence -- Extracted sequence from in-silico PCR
        vf_strand -- Which strand the extracted VF was identified on.
                     This is used to determine whether reverse complement
                     is needed.

    Returns:
        identity -- Percent identity between VF sequence and reference
        coverage_percent_length -- Length of vf_sequence divided by the length of the reference
        coverage_alignment -- Alternative coverage metric that ignores gaps. Length of vf_sequence minus gaps divided by length of the reference.
    """

     # Create pairwise alignment of the reference and vf_sequence
    aligner = PairwiseAligner(scoring="blastn")
    aligner.mode = 'global'

    # Reverse complement if - strand
    if vf_strand == "-":
        vf_sequence = reverse_complement(vf_sequence)

    # Grab the best alignment
    alignment = aligner.align(reference_sequence, vf_sequence)[0]
    # Number of matches is the number of | characters in the printout
    matches = alignment.format().count("|")
    # Identity is the number of matches over the total length, multiply by 100 for %
    identity = round(matches/alignment.length*100, 2)
    # Simple coverage metric that looks at length discrepency
    coverage_percent_length = round(len(vf_sequence)/len(reference_sequence)*100, 2)
    # Alternative coverage metric that does not count gaps
    coverage_alignment = round((len(vf_sequence) - alignment[1].count("-"))/len(reference_sequence)*100, 2)

    return identity, coverage_percent_length, coverage_alignment


def validate_vf(identity, coverage_percent_length, length_limit, identity_limit):
    """
    Validates that a VF meets criteria to be called.

    Arguments:
        reference_sequence -- Target virulence factor sequence
        vf_sequence -- Extracted sequence from in-silico PCR
        vf_strand -- Which strand the extracted VF was identified on.
                     This is used to determine whether reverse complement
                     is needed.
        length_limit -- Argument for length limit provided by the user.
                        This is written as a percent +/- the length of
                        the reference sequence.
        identity_limit -- Argument for identity limit provided by the user.

    Return:
        valid -- True or false if valid or not
        error -- Reason that VF was not validated
    """
    valid = False
    error = ""
    # Check that identity and length limits are met
    if identity >= identity_limit and coverage_percent_length >= 100 - length_limit and coverage_percent_length <= 100 + length_limit:
        error = ""
        valid = True
    # Identity meets criteria, but not length
    elif identity >= identity_limit and not (coverage_percent_length >= 100 - length_limit and coverage_percent_length <= 100 + length_limit):
        error = "Length limit not satisfied."
    # Length meets criteria, but not identity
    elif coverage_percent_length >= 100 - length_limit and coverage_percent_length <= 100 + length_limit and not identity >= identity_limit:
        error = "Identity limit not satisfied."
    else:
        error = "Identity and length limits not satisfied."
    
    return valid, error

def reverse_complement(sequence):
    """
    Reverse complements a sequence.

    Arguments:
        sequence -- DNA sequence to be reverse complemented

    Returns:
        reverse_complement -- Reverse complement of the sequence
    """
    # Convert to bioconda sequence object
    sequence = Seq(sequence)
    
    return sequence.reverse_complement()

def find_overlaps(table):
    """
    Identifies overlapping sequences and adds warning messages when overlaps are identified.

    Arguments:
        table -- Dataframe with results from SPIDER

    Returns:
        table -- Input table with overlapping regions appended to message
    """
    # Iterate over all unique pairs
    for idx1, idx2 in combinations(table.index, 2):
        row1, row2 = table.loc[idx1], table.loc[idx2]

        # Check conditions
        if row1["Valid"] and row2["Valid"] and row1["Query"] == row2["Query"] and row1["Strand"] == row2["Strand"] and row1["Contig"] == row2["Contig"]:
            # Check for overlap
            if row1["End"] >= row2["Start"] and row2["End"] >= row1["Start"]:
                warning1 = f"WARNING: This sequence overlaps the same region as {row2['Name']}"
                warning2 = f"WARNING: This sequence overlaps the same region as {row1['Name']}"

                # Append warning to both rows
                for idx, warning in [(idx1, warning1), (idx2, warning2)]:
                    if table.at[idx, "Message"]:
                        table.at[idx, "Message"] += "; " + warning
                    else:
                        table.at[idx, "Message"] = warning

    return table
