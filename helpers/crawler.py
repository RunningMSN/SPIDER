import uuid
import os
import shutil
import subprocess
import math
from helpers.settings import blast_columns_fmt_6
import pandas as pd
import numpy as np
from pyfaidx import Fasta
from Bio.Align import PairwiseAligner
from Bio.Seq import Seq

def crawl(fasta, db_loc, slide_limit, length_limit, identity_limit, primer_size):
    """
    Runs SPIDER to identify VFs in the supplied fasta file.

    Arguments:

    """
    print(f"Beginning to crawl {fasta} using the following settings:")
    print(f"Primer Size: {primer_size}bp")
    print(f"Slide Limit: {slide_limit}%")
    print(f"Length Limit: {length_limit}%")
    print(f"Identity Limit: {identity_limit}%")

    # Create a temporary directory
    temp_directory = f"spider_tmp_{uuid.uuid4().hex}"

    # Setup crawler environment
    setup(fasta, temp_directory)

    # Iterate through all VFs to test
    with open(db_loc, "r") as database:
        # Load VFs by header and sequence
        for header, sequence in zip(database, database):
            print(f"Testing {header}")
            identify_vf(header, sequence.strip(), slide_limit, primer_size, temp_directory, length_limit, identity_limit)

    # Cleanup temporary environment
    cleanup(temp_directory)

def setup(fasta, temp_directory):
    """
    Sets up a working environment for SPIDER.

    Arguments:
        fasta -- Location of the assembly being searched
        temp_directory -- Location of temporary directory to be made
    """
    # Create temporary directory
    os.makedirs(temp_directory)

    # Copy assembly to the temp directory
    shutil.copy(fasta, f"{temp_directory}/reference.fasta")

    # Make blast DB for primer lookup
    makeblastdb_cmd = ["makeblastdb", "-in", f"{temp_directory}/reference.fasta", 
                       "-dbtype", "nucl"]
    subprocess.run(makeblastdb_cmd)


def cleanup(temp_directory):
    """
    Cleans up the temporary files generated by this script.

    Arguments:
        temp_directory -- Temporary SPIDER working directory that 
                          will be removed.
    """
    # Remove temporary directory
    shutil.rmtree(temp_directory)


def identify_vf(header, ref_sequence, slide_limit, primer_size, temp_directory, length_limit, identity_limit):
    """
    Identifies the virulence factor if present.
    """
    # Make directory for the VF
    vf_directory = f"{temp_directory}/{header.split(' ')[0].replace('>','')}"
    os.makedirs(vf_directory)

    # Find sequence length for number of primers to generate
    primer_slide_limit_nt = math.floor(slide_limit / 100 * len(ref_sequence))

    # Number of primers to generate is primer size subtracted from the limit
    number_primers = primer_slide_limit_nt - primer_size

    # Generate the forward primers
    with open(f"{vf_directory}/forward_primers.fasta", "w") as forward_primers:
        for i in range(0, number_primers):
            forward_primers.write(f">forward_{i}\n{ref_sequence[i:i+primer_size]}\n")

    # Generate the reverse primers
    with open(f"{vf_directory}/reverse_primers.fasta", "w") as reverse_primers:
        for i in range(0, number_primers):
            reverse_primers.write(f">reverse_{i}\n{ref_sequence[len(ref_sequence)-i-primer_size:len(ref_sequence)-i]}\n")

    # BLAST both sets of primers
    for primer_set in ("forward_primers", "reverse_primers"):
        blast_cmd = ["blastn", "-query", f"{vf_directory}/{primer_set}.fasta", 
                     "-db", f"{temp_directory}/reference.fasta", 
                     "-outfmt", "6", "-word_size", f"{primer_size}", 
                     "-out", f"{vf_directory}/{primer_set}.blast.txt"]
        subprocess.run(blast_cmd)
        
    # Obtain primer matches
    forward_matches, reverse_matches = parse_primer_matches(vf_directory, len(ref_sequence), temp_directory)
    # Sort the primers into pairs
    primer_pairs, errors = sort_primer_pairs(forward_matches, reverse_matches, len(ref_sequence))
    # Extract VF sequence for each primer pair
    if len(primer_pairs) > 0:
        with open(f"{vf_directory}/vf_sequences.fasta", "w") as sequence_output_file:
            vf_extracted_counter = 0
            for pair in primer_pairs:
                contig, start, end, strand, forward_slide, reverse_slide = extract_vf_location(pair, forward_matches, reverse_matches)

                # Extract the VF sequence
                vf_sequence, vf_length = extract_vf_sequence(contig, start, end, temp_directory)
                
                # Align the VF to get identity and coverage
                identity, coverage_percent_length, coverage_alignment = align_vf(ref_sequence, vf_sequence, strand)


                

                # sequence_output_file.write(f">{vf_extracted_counter}\n{vf_sequence}\n")
                print(f"Contig: {contig}\tStart: {start}\tF Slide: {forward_slide}\tEnd: {end}\tR Slide: {reverse_slide}\t{strand}\tLength: {vf_length}\tExpected Length: {len(ref_sequence)}")


def parse_primer_matches(vf_directory, expected_vf_length, temp_directory):
    """
    """
    # Parse the best forward primer match(es)
    forward_matches = pd.read_csv(f"{vf_directory}/forward_primers.blast.txt", sep="\t", header=None, names=blast_columns_fmt_6)
    if len(forward_matches) > 0:
        forward_matches["qseqid"] = forward_matches["qseqid"].str.replace("forward_", "")
        # Sort to make sure first primers are kept
        forward_matches["qseqid"] = forward_matches["qseqid"].astype(int)
        forward_matches.sort_values(by="qseqid", ascending = True, inplace= True)
        # Keep only matches for the best primer
        forward_matches = forward_matches[forward_matches["qseqid"] == forward_matches["qseqid"][0]]
        # Add information about strand
        forward_matches["strand"] = np.where(forward_matches["sstart"] < forward_matches["send"], "+", "-")

    # If no matches found, set to null
    else:
        forward_matches = None
    
    # Parse the best reverse primer match(es)
    reverse_matches = pd.read_csv(f"{vf_directory}/reverse_primers.blast.txt", sep="\t", header=None, names=blast_columns_fmt_6)
    if len(reverse_matches) > 0:
        reverse_matches["qseqid"] = reverse_matches["qseqid"].str.replace("reverse_", "")
        # Sort to make sure first primers are kept
        reverse_matches["qseqid"] = reverse_matches["qseqid"].astype(int)
        reverse_matches.sort_values(by="qseqid", ascending = True, inplace= True)
        # Keep only matches for the best primer
        reverse_matches = reverse_matches[reverse_matches["qseqid"] == reverse_matches["qseqid"][0]]
        # Add information about strand
        reverse_matches["strand"] = np.where(reverse_matches["sstart"] < reverse_matches["send"], "+", "-")

    # If no matches found, set to null
    else:
        reverse_matches = None

    return forward_matches, reverse_matches

    
        

    # TODO: Add validation for the primer pairs

    # TODO: Add extraction of the VF sequence


def sort_primer_pairs(forward_matches, reverse_matches, expected_vf_length):
    """
    Identifies primer pairs. The total number of pairs will be whichever 
    direction primer had less hits. E.g. if forward primer was found once, but
    reverse was found twice, one pair of primers will be calculated.

    No validation of these primer matches is performed here. Pairs will be 
    validated in a separate function before finalizing VF call.

    Arguments:
        forward_matches -- Pandas dataframe containing forward primer matches
        reverse_matches -- Pandas dataframe containing the reverse primer matches
        expected_vf_length -- Expected length of the VF is the length of the reference
                              VF sequence.

    Returns:
        primer_pairs_indices -- List of tuples containing indices of the 
        forward and reverse primers that form pairs
        errors -- List of errors that describes why VF failed to be identified
    """
    # Store pairs as tuples of forward and reverse index
    primer_pairs_indices = []
    errors = []
    
    # Identify primer pairings where smallest number of primers in one set are paired with primers from other set
    if forward_matches is not None and reverse_matches is not None: 
        # Set positions to integers
        forward_matches["sstart"] = forward_matches["sstart"].astype(int)
        forward_matches["send"] = forward_matches["send"].astype(int)
        reverse_matches["sstart"] = reverse_matches["sstart"].astype(int)
        reverse_matches["send"] = reverse_matches["send"].astype(int)

        # Store indices before getting merged
        forward_matches["index"] = forward_matches.index
        reverse_matches["index"] = reverse_matches.index 
        
        # Merge on sseqid and strand to make sure primers are on correct contig and in correct direction
        pairs = pd.merge(forward_matches, reverse_matches, on=["sseqid", "strand"], suffixes=("_f", "_r"))

        # Calculate distance from expected length to get primer pairs distances
        if len(pairs) > 0:
            # Calculate distances
            pairs["distance"] = abs(abs(pairs["sstart_f"] - pairs["send_r"]) - expected_vf_length)
            
            # Sort by distance
            pairs.sort_values(by="distance", ascending = True, inplace= True)

            # Number of primer pairs to target is the lesser of number of primers identified in forward or reverse direction
            target_pairs_count = min(len(forward_matches), len(reverse_matches))
            used_forward = set()
            used_reverse = set()
            # Number of pairs identified so far
            pairs_count = 0
            
            # Iterate throw each pair by ascending distance
            for index, row in pairs.iterrows():
                if pairs_count < target_pairs_count:
                    # If current smallest distance, and have not used this primer pair, then add it in
                    if not row["index_f"] in used_forward and not row["index_r"] in used_reverse:
                        used_forward.add(row["index_f"])
                        used_reverse.add(row["index_r"])
                        primer_pairs_indices.append((row["index_f"],row["index_r"]))
                # If have satisfied target number of primer pairs, then break from loop
                else:
                    break

    elif forward_matches is None and reverse_matches is None:
        errors.append("Both forward and reverse primers were not identified")
    elif forward_matches is None:
        errors.append(f"The forward primer was not identified, a reverse primer was found.") # TODO: Add slide amount that identified the found primer
    elif reverse_matches is None:
        errors.append(f"The reverse primer was not identified, a forward primer was found.") # TODO: Add slide amount that identified the found primer
    return primer_pairs_indices, errors


def extract_vf_location(primer_pair_indices, forward_matches, reverse_matches):
    """
    Returns the location of the virulence factor given a set of primer pair indices
    for the forward and reverse BLAST searches.

    Arguments:
        primer_pair_indiced -- Tuple of indices for the forward and reverse
                               BLAST matches for the primers.
        forward_matches -- Pandas dataframe containing forward primer matches
        reverse_matches -- Pandas dataframe containing the reverse primer matches

    Return:
        contig -- Contig on which VF is located
        start -- Start position
        end -- End position
        strand -- +/- strand
        forward_slide -- # of bases slide on forward primer
        reverse_slide -- # of bases slide on reverse primer
    """
    # Obtain location of VF
    contig = forward_matches.iloc[primer_pair_indices[0]]["sseqid"]
    strand = forward_matches.iloc[primer_pair_indices[0]]["strand"]
    
    ## If positive strand then going from sstart to send
    if strand == "+":
        # Start position is where primer landed and subtract off primer slide amount
        start = int(forward_matches.iloc[primer_pair_indices[0]]["sstart"]) - int(forward_matches.iloc[primer_pair_indices[0]]["qseqid"])
        # End position is where primer landed and add primer slide amount
        end = int(reverse_matches.iloc[primer_pair_indices[1]]["send"]) + int(reverse_matches.iloc[primer_pair_indices[1]]["qseqid"])
    # If negative strand, then reverse the direction
    else:
        # Start position is where reverse primer landed and add slide amount
        start = int(reverse_matches.iloc[primer_pair_indices[1]]["send"]) + int(reverse_matches.iloc[primer_pair_indices[1]]["qseqid"])
        # End position is where forward primer landed, and subtract slide amount
        end = int(forward_matches.iloc[primer_pair_indices[0]]["sstart"]) - int(forward_matches.iloc[primer_pair_indices[0]]["qseqid"])
    
    forward_slide = forward_matches.iloc[primer_pair_indices[0]]["qseqid"]
    reverse_slide = reverse_matches.iloc[primer_pair_indices[1]]["qseqid"]

    return contig, start, end, strand, forward_slide, reverse_slide


def extract_vf_sequence(contig, start, end, temp_directory):
    """
    Extracts the virulence factor sequence using pyfaidx.

    Arguments:
        contig -- Contig on which VF is located.
        start -- Start position
        end -- End position
        temp_directory -- Working directory for SPIDER

    Returns:
        seq -- Virulence factor sequence that was identified
        length -- Length of the virulence factor sequence extracted
    """
    genome = Fasta(f"{temp_directory}/reference.fasta")
    contig = str(contig)
    # Must subtract 1 base from start since python index at 0 and BLAST coordinate index at 1
    seq = str(genome[contig][start-1:end])
    length = end-start+1 # Add 1 to be inclusive of ends
    
    return seq, length


def align_vf(reference_sequence, vf_sequence, vf_strand):
    """
    Aligns VF sequence to the reference sequence.

    Arguments:
        reference_sequence -- Target virulence factor sequence
        vf_sequence -- Extracted sequence from in-silico PCR
        vf_strand -- Which strand the extracted VF was identified on.
                     This is used to determine whether reverse complement
                     is needed.

    Returns:
        identity -- Percent identity between VF sequence and reference
        coverage_percent_length -- Length of vf_sequence divided by the length of the reference
        coverage_alignment -- Alternative coverage metric that ignores gaps. Length of vf_sequence minus gaps divided by length of the reference.
    """

     # Create pairwise alignment of the reference and vf_sequence
    aligner = PairwiseAligner(scoring="blastn")
    aligner.mode = 'global'

    # Reverse complement if - strand
    if vf_strand == "-":
        vf_sequence = reverse_complement(vf_sequence)

    # Grab the best alignment
    alignment = aligner.align(reference_sequence, vf_sequence)[0]
    # Number of matches is the number of | characters in the printout
    matches = alignment.format().count("|")
    # Identity is the number of matches over the total length, multiply by 100 for %
    identity = matches/alignment.length*100
    # Simple coverage metric that looks at length discrepency
    coverage_percent_length = len(vf_sequence)/len(reference_sequence)*100
    # Alternative coverage metric that does not count gaps
    coverage_alignment = (len(vf_sequence) - alignment[1].count("-"))/len(reference_sequence)*100

    return identity, coverage_percent_length, coverage_alignment

def validate_vf(reference_sequence, identity, coverage_percent_length, length_limit, identity_limit):
    """
    Validates that a VF meets criteria to be called.

    Arguments:
        reference_sequence -- Target virulence factor sequence
        vf_sequence -- Extracted sequence from in-silico PCR
        vf_strand -- Which strand the extracted VF was identified on.
                     This is used to determine whether reverse complement
                     is needed.
        length_limit -- Argument for length limit provided by the user.
                        This is written as a percent +/- the length of
                        the reference sequence.
        identity_limit -- Argument for identity limit provided by the user.

    Return:
        valid -- True or false if valid or not
        error -- Reason that VF was not validated
    """
    # Check that identity and length limits are met
    if identity >= identity_limit and coverage_percent_length >= 100 - length_limit and coverage_percent_length <= 100 + length_limit:
        return True
    # Identity meets criteria, but not length
    elif identity >= identity_limit and not (coverage_percent_length >= 100 - length_limit and coverage_percent_length <= 100 + length_limit):
        error = "Length limit not satisfied."
        return False
    # Length meets criteria, but not identity
    elif coverage_percent_length >= 100 - length_limit and coverage_percent_length <= 100 + length_limit and not identity >= identity_limit:
        error = "Identity limit not satisfied."
        return False
    else:
        error = "Identity and length limits not satisfied."
        return False


def reverse_complement(sequence):
    """
    Reverse complements a sequence.

    Arguments:
        sequence -- DNA sequence to be reverse complemented

    Returns:
        reverse_complement -- Reverse complement of the sequence
    """
    # Convert to bioconda sequence object
    sequence = Seq(sequence)
    
    return sequence.reverse_complement()