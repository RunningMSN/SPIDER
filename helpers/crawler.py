import uuid
import os
import shutil
import subprocess
import math
from helpers.settings import BLAST_COLUMNS_FMT_6, SPIDER_RESULTS_COLUMNS, GFF3_COLUMNS
import pandas as pd
import numpy as np
from pyfaidx import Fasta
from Bio.Align import PairwiseAligner
from Bio.Seq import Seq
from Bio import SeqIO
from itertools import combinations
import re
import sys

def crawl(fasta, db_loc, slide_limit, length_limit, identity_limit, primer_size, check_overlaps, check_start_stop, annotation):
    """
    Runs SPIDER to identify targets in the supplied fasta file.

    Arguments:
        fasta -- Location of assembly to query
        db_loc -- Location of target datavase
        slide_limit -- Percentage of target gene that SPIDER can slide
        length_limit -- Percentage limit of length for which a target will validate
        identity_limit -- Threshold identity at which to call a target as present
        primer_size -- Size of primer for in-silico PCR
        check_overlap -- True/false check if amplicons in same sample are overlapping
        check_start_stop -- True/false check for closest start/stop codons near the extracted amplicon

    Returns:
        df_results -- Results of crawler in the form of pandas dataframe
    """
    # Create a temporary directory name
    temp_directory = f"spider_tmp_{uuid.uuid4().hex}"

    # Setup crawler environment and temp directory
    setup(fasta, temp_directory)

    # Iterate through all targets to test
    all_results = []
    with open(db_loc, "r") as database:
        # Load targets by header and sequence
        for header, sequence in zip(database, database):
            results = identify_target(header, sequence.strip(), slide_limit, primer_size, temp_directory, length_limit, identity_limit)
            for result in results:
                # Add header to the result as first item
                result = (fasta,header.strip().replace(">",""),) + result
                # Append to overall results
                all_results.append(result)
    spider_results = pd.DataFrame(all_results, columns=SPIDER_RESULTS_COLUMNS)

    # Add warnings for overlaps
    if check_overlaps:
        spider_results = find_overlaps(spider_results)
    # Add start and stop codons
    if check_start_stop:
        spider_results = find_start_stop(spider_results, temp_directory)
    if annotation:
        spider_results = find_annotations(spider_results, annotation, temp_directory)


    # Cleanup temporary environment
    cleanup(temp_directory)

    # Return results
    return spider_results

def setup(fasta, temp_directory):
    """
    Sets up a working environment for SPIDER.

    Arguments:
        fasta -- Location of the assembly being searched
        temp_directory -- Location of temporary directory to be made
    """
    # Create temporary directory
    os.makedirs(temp_directory)

    # Copy assembly to the temp directory
    shutil.copy(fasta, f"{temp_directory}/reference.fasta")

    # Make blast DB for primer lookup
    makeblastdb_cmd = ["makeblastdb", "-in", f"{temp_directory}/reference.fasta", 
                       "-dbtype", "nucl"]
    subprocess.run(makeblastdb_cmd, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)


def cleanup(temp_directory):
    """
    Cleans up the temporary files generated by this script.

    Arguments:
        temp_directory -- Temporary SPIDER working directory that 
                          will be removed.
    """
    # Remove temporary directory
    shutil.rmtree(temp_directory)


def identify_target(header, ref_sequence, slide_limit, primer_size, temp_directory, length_limit, identity_limit):
    """
    Identifies the target sequence if present.

    Arguments:
        header -- target header
        ref_sequence -- target reference sequence
        slide_limit -- User set slide limit for primers
        primer_size -- User provided primer length
        temp_directory -- Temporary directory to use
        length_limit -- User provided limit on length to use
        identity_limit -- User provided identity limit to use

    Returns:
        results -- List of tuples that contain results. Each tuple is in the format: 
                   (Valid, Contig, Start, F_Slide, End, R_Slide, Strand, Identity, Target_length, 
                   Ref_Length, Coverage_Perc_Len, Coverage_Perc_Align, Message)
    """
    # Make directory for the target
    target_directory = f"{temp_directory}/{header.split(' ')[0].replace('>','')}"
    os.makedirs(target_directory)

    # Find sequence length for number of primers to generate
    ref_length = len(ref_sequence)
    number_primers = math.floor(slide_limit / 100 * ref_length)
    # Make sure that the number of primers can never be 0
    if number_primers < 1: number_primers = 1

    # Generate the forward primers
    with open(f"{target_directory}/forward_primers.fasta", "w") as forward_primers:
        for i in range(0, number_primers):
            forward_primers.write(f">forward_{i}\n{ref_sequence[i:i+primer_size]}\n")

    # Generate the reverse primers
    with open(f"{target_directory}/reverse_primers.fasta", "w") as reverse_primers:
        for i in range(0, number_primers):
            reverse_primers.write(f">reverse_{i}\n{ref_sequence[ref_length-i-primer_size:ref_length-i]}\n")

    # BLAST both sets of primers
    for primer_set in ("forward_primers", "reverse_primers"):
        blast_cmd = ["blastn", "-query", f"{target_directory}/{primer_set}.fasta", 
                     "-db", f"{temp_directory}/reference.fasta", 
                     "-outfmt", "6", "-word_size", f"{primer_size}", 
                     "-out", f"{target_directory}/{primer_set}.blast.txt"]
        subprocess.run(blast_cmd)
        
    # Obtain primer matches
    forward_matches, reverse_matches = parse_primer_matches(target_directory)
    # Sort the primers into pairs
    primer_pairs, error = sort_primer_pairs(forward_matches, reverse_matches, ref_length)
    # Store returned output
    results = []

    # Extract target sequence for each primer pair
    if len(primer_pairs) > 0:
        target_extracted_counter = 0
        for pair in primer_pairs:
            contig, start, end, strand, forward_slide, reverse_slide = extract_target_location(pair, forward_matches, reverse_matches, temp_directory)
            
            # Extract the target sequence
            target_sequence, target_length = extract_target_sequence(contig, start, end, temp_directory)
            
            # Align the target to get identity and coverage
            identity, coverage_percent_length, coverage_alignment = align_target(ref_sequence, target_sequence, strand)
            
            # Check validity of target
            valid, error = validate_target(identity, coverage_percent_length, length_limit, identity_limit)

            # Add tuple for output: (Valid, Start, F_Slide, End, R_Slide, Strand, Identity, target_length, Ref_Length, Coverage_Perc_Len, Coverage_Perc_Align, Error Message)
            results.append((valid, contig, start, forward_slide, end, reverse_slide, strand, identity, target_length, ref_length, coverage_percent_length, coverage_alignment, error))
    else:
        results.append((False, "NA", "NA", "NA", "NA", "NA", "NA", "NA", "NA", ref_length, "NA", "NA", error))

    return results

def parse_primer_matches(target_directory):
    """
    Identifies the best primer match for target.

    Arguments:
        target_directory -- Temporary directory being used for the target
    
    Returns:
        forward_matches - Pandas dataframe with best forward primer matches
        reverse_matches - Pandas dataframe with best reverse primer matches
    """
    # Parse the best forward primer match(es)
    forward_matches = pd.read_csv(f"{target_directory}/forward_primers.blast.txt", sep="\t", header=None, names=BLAST_COLUMNS_FMT_6)
    if len(forward_matches) > 0:
        # Set names to just the slide amount
        forward_matches["qseqid"] = forward_matches["qseqid"].str.replace("forward_", "")
        # Sort to make sure first primers are kept
        forward_matches["qseqid"] = forward_matches["qseqid"].astype(int)
        forward_matches.sort_values(by="qseqid", ascending = True, inplace= True)
        # Keep only matches for the best primer
        forward_matches = forward_matches[forward_matches["qseqid"] == forward_matches["qseqid"][0]]
        # Add information about strand
        forward_matches["strand"] = np.where(forward_matches["sstart"] < forward_matches["send"], "+", "-")

    # If no matches found, set to null
    else:
        forward_matches = None
    
    # Parse the best reverse primer match(es)
    reverse_matches = pd.read_csv(f"{target_directory}/reverse_primers.blast.txt", sep="\t", header=None, names=BLAST_COLUMNS_FMT_6)
    if len(reverse_matches) > 0:
        # Set names to just the slide amount
        reverse_matches["qseqid"] = reverse_matches["qseqid"].str.replace("reverse_", "")
        # Sort to make sure first primers are kept
        reverse_matches["qseqid"] = reverse_matches["qseqid"].astype(int)
        reverse_matches.sort_values(by="qseqid", ascending = True, inplace= True)
        # Keep only matches for the best primer
        reverse_matches = reverse_matches[reverse_matches["qseqid"] == reverse_matches["qseqid"][0]]
        # Add information about strand
        reverse_matches["strand"] = np.where(reverse_matches["sstart"] < reverse_matches["send"], "+", "-")

    # If no matches found, set to null
    else:
        reverse_matches = None

    return forward_matches, reverse_matches


def sort_primer_pairs(forward_matches, reverse_matches, expected_target_length):
    """
    Identifies primer pairs. The total number of pairs will be whichever 
    direction primer had less hits. E.g. if forward primer was found once, but
    reverse was found twice, one pair of primers will be calculated.

    No validation of these primer matches is performed here. Pairs will be 
    validated in a separate function before finalizing target call.

    Arguments:
        forward_matches -- Pandas dataframe containing forward primer matches
        reverse_matches -- Pandas dataframe containing the reverse primer matches
        expected_target_length -- Expected length of the target is the length of the reference
                              target sequence.

    Returns:
        primer_pairs_indices -- List of tuples containing indices of the 
        forward and reverse primers that form pairs
        error -- Reason why target failed to be identified
    """
    # Store pairs as tuples of forward and reverse index
    primer_pairs_indices = []
    # Store error message
    error = ""

    # Identify primer pairings where smallest number of primers in one set are paired with primers from other set
    if forward_matches is not None and reverse_matches is not None: 
        # Set positions to integers
        forward_matches["sstart"] = forward_matches["sstart"].astype(int)
        forward_matches["send"] = forward_matches["send"].astype(int)
        reverse_matches["sstart"] = reverse_matches["sstart"].astype(int)
        reverse_matches["send"] = reverse_matches["send"].astype(int)

        # Store indices before getting merged
        forward_matches["index"] = forward_matches.index
        reverse_matches["index"] = reverse_matches.index 
        
        # Merge on sseqid and strand to make sure primers are on correct contig and in correct direction
        pairs = pd.merge(forward_matches, reverse_matches, on=["sseqid", "strand"], suffixes=("_f", "_r"))

        if len(pairs) == 0:
            error = f"Forward primers found on {','.join(pd.unique(forward_matches['sseqid'].astype(str)))} ({'/'.join(forward_matches['strand'])}) and reverse primers found on {','.join(pd.unique(reverse_matches['sseqid'].astype(str)))} ({'/'.join(reverse_matches['strand'])}) "

        # Filter out bad pairs that are in improper order
        valid_ordered_pairs = pairs[
            ((pairs["strand"] == "+") & (pairs["sstart_f"] < pairs["send_r"])) |
            ((pairs["strand"] == "-") & (pairs["send_r"] < pairs["sstart_f"]))
        ].copy()

        # Calculate distance from expected length to get primer pairs distances
        if len(valid_ordered_pairs) > 0:
            # Calculate distances
            valid_ordered_pairs["distance"] = abs(abs(valid_ordered_pairs["sstart_f"] - valid_ordered_pairs["send_r"]) - expected_target_length)
            
            # Sort by distance
            valid_ordered_pairs.sort_values(by="distance", ascending = True, inplace= True)

            # Number of primer pairs to target is the lesser of number of primers identified in forward or reverse direction
            target_pairs_count = min(len(forward_matches), len(reverse_matches))
            used_forward = set()
            used_reverse = set()
            # Number of pairs identified so far
            pairs_count = 0
            
            # Iterate throw each pair by ascending distance
            for index, row in valid_ordered_pairs.iterrows():
                if pairs_count < target_pairs_count:
                    # If current smallest distance, and have not used this primer pair
                    if not row["index_f"] in used_forward and not row["index_r"] in used_reverse and row["distance"] > 0:
                        used_forward.add(row["index_f"])
                        used_reverse.add(row["index_r"])
                        primer_pairs_indices.append((row["index_f"],row["index_r"]))
                # If have satisfied target number of primer pairs, then break from loop
                else:
                    break
            # Error is empty
            error = ""
        elif len(pairs) > 0:
            error = "Forward and reverse primers were identified, but they were not in the correct order (i.e. F after R or R after F)."
    elif forward_matches is None and reverse_matches is None:
        error = "Neither forward nor reverse primers were not identified."
    elif forward_matches is None:
        error = f"The forward primer was not identified, a reverse primer was found with slide of {reverse_matches["qseqid"][0]}."
    elif reverse_matches is None:
        error = f"The reverse primer was not identified, a forward primer was found with slide of {forward_matches["qseqid"][0]}."

    return primer_pairs_indices, error


def extract_target_location(primer_pair_indices, forward_matches, reverse_matches, temp_directory):
    """
    Returns the location of the target given a set of primer pair indices
    for the forward and reverse BLAST searches.

    Arguments:
        primer_pair_indiced -- Tuple of indices for the forward and reverse
                               BLAST matches for the primers.
        forward_matches -- Pandas dataframe containing forward primer matches
        reverse_matches -- Pandas dataframe containing the reverse primer matches

    Return:
        contig -- Contig on which target is located
        start -- Start position
        end -- End position
        strand -- +/- strand
        forward_slide -- # of bases slide on forward primer
        reverse_slide -- # of bases slide on reverse primer
    """
    # Obtain location of target
    contig = forward_matches.iloc[primer_pair_indices[0]]["sseqid"]
    strand = forward_matches.iloc[primer_pair_indices[0]]["strand"]
    
    ## If positive strand then going from sstart to send
    if strand == "+":
        # Start position is where primer landed and subtract off primer slide amount
        start = int(forward_matches.iloc[primer_pair_indices[0]]["sstart"]) - int(forward_matches.iloc[primer_pair_indices[0]]["qseqid"])
        # End position is where primer landed and add primer slide amount
        end = int(reverse_matches.iloc[primer_pair_indices[1]]["send"]) + int(reverse_matches.iloc[primer_pair_indices[1]]["qseqid"])
    # If negative strand, then reverse the direction
    else:
        # Start position is where reverse primer landed and add slide amount
        start = int(reverse_matches.iloc[primer_pair_indices[1]]["send"]) - int(reverse_matches.iloc[primer_pair_indices[1]]["qseqid"])
        # End position is where forward primer landed, and subtract slide amount
        end = int(forward_matches.iloc[primer_pair_indices[0]]["sstart"]) + int(forward_matches.iloc[primer_pair_indices[0]]["qseqid"])
    

    # Grab length of the contig to ensure that sliding doesn't exceed the ends
    contigs = SeqIO.index(f"{temp_directory}/reference.fasta", "fasta")
    contig_length = len(contigs[str(contig)].seq)
    
    # Check that not exceeding the contig limits
    if start < 1:
        start = 1
    if end > contig_length:
        end = contig_length

    forward_slide = forward_matches.iloc[primer_pair_indices[0]]["qseqid"]
    reverse_slide = reverse_matches.iloc[primer_pair_indices[1]]["qseqid"]

    return contig, start, end, strand, forward_slide, reverse_slide


def extract_target_sequence(contig, start, end, temp_directory):
    """
    Extracts the target sequence using pyfaidx.

    Arguments:
        contig -- Contig on which target is located.
        start -- Start position
        end -- End position
        temp_directory -- Working directory for SPIDER

    Returns:
        seq -- Target sequence that was identified
        length -- Length of the target sequence extracted
    """
    genome = Fasta(f"{temp_directory}/reference.fasta")
    contig = str(contig)
    # Must subtract 1 base from start since python index at 0 and BLAST coordinate index at 1
    seq = str(genome[contig][start-1:end])
    length = end-start+1 # Add 1 to be inclusive of ends
    
    return seq, length


def align_target(reference_sequence, target_sequence, target_strand):
    """
    Aligns target sequence to the reference sequence.

    Arguments:
        reference_sequence -- Target sequence
        target_sequence -- Extracted sequence from in-silico PCR
        target_strand -- Which strand the extracted target was identified on.
                     This is used to determine whether reverse complement
                     is needed.

    Returns:
        identity -- Percent identity between target sequence and reference
        coverage_percent_length -- Length of target_sequence divided by the length of the reference
        coverage_alignment -- Alternative coverage metric that ignores gaps. Length of target_sequence minus gaps divided by length of the reference.
    """

     # Create pairwise alignment of the reference and target_sequence
    aligner = PairwiseAligner(scoring="blastn")
    aligner.mode = 'global'

    # Reverse complement if - strand
    if target_strand == "-":
        target_sequence = reverse_complement(target_sequence)

    # Grab the best alignment
    alignment = aligner.align(reference_sequence, target_sequence)[0]
    # Number of matches is the number of | characters in the printout
    matches = alignment.format().count("|")
    # Identity is the number of matches over the total length, multiply by 100 for %
    identity = round(matches/alignment.length*100, 2)
    # Simple coverage metric that looks at length discrepency
    coverage_percent_length = round(len(target_sequence)/len(reference_sequence)*100, 2)
    # Alternative coverage metric that does not count gaps
    coverage_alignment = round((len(target_sequence) - alignment[1].count("-"))/len(reference_sequence)*100, 2)

    return identity, coverage_percent_length, coverage_alignment


def validate_target(identity, coverage_percent_length, length_limit, identity_limit):
    """
    Validates that a target meets criteria to be called.

    Arguments:
        reference_sequence -- Target sequence
        target_sequence -- Extracted sequence from in-silico PCR
        target_strand -- Which strand the extracted target was identified on.
                     This is used to determine whether reverse complement
                     is needed.
        length_limit -- Argument for length limit provided by the user.
                        This is written as a percent +/- the length of
                        the reference sequence.
        identity_limit -- Argument for identity limit provided by the user.

    Return:
        valid -- True or false if valid or not
        error -- Reason that target was not validated
    """
    valid = False
    error = ""
    # Check that identity and length limits are met
    if identity >= identity_limit and coverage_percent_length >= 100 - length_limit and coverage_percent_length <= 100 + length_limit:
        error = ""
        valid = True
    # Identity meets criteria, but not length
    elif identity >= identity_limit and not (coverage_percent_length >= 100 - length_limit and coverage_percent_length <= 100 + length_limit):
        error = "Length limit not satisfied."
    # Length meets criteria, but not identity
    elif coverage_percent_length >= 100 - length_limit and coverage_percent_length <= 100 + length_limit and not identity >= identity_limit:
        error = "Identity limit not satisfied."
    else:
        error = "Identity and length limits not satisfied."
    
    return valid, error

def reverse_complement(sequence):
    """
    Reverse complements a sequence.

    Arguments:
        sequence -- DNA sequence to be reverse complemented

    Returns:
        reverse_complement -- Reverse complement of the sequence
    """
    # Convert to bioconda sequence object
    sequence = Seq(sequence)
    
    return sequence.reverse_complement()

def find_overlaps(table):
    """
    Identifies overlapping sequences and adds warning messages when overlaps are identified.

    Arguments:
        table -- Dataframe with results from SPIDER

    Returns:
        table -- Input table with overlapping regions appended to message
    """
    table["Overlap"] = ""
    # Iterate over all unique pairs
    for idx1, idx2 in combinations(table.index, 2):
        row1, row2 = table.loc[idx1], table.loc[idx2]

        # Check conditions
        if row1["Valid"] and row2["Valid"] and row1["Query"] == row2["Query"] and row1["Strand"] == row2["Strand"] and row1["Contig"] == row2["Contig"]:
            # Check for overlap
            if row1["End"] >= row2["Start"] and row2["End"] >= row1["Start"]:
                warning1 = f"{row2['Name']}"
                warning2 = f"{row1['Name']}"

                # Append warning to both rows
                for idx, warning in [(idx1, warning1), (idx2, warning2)]:
                    if table.at[idx, "Overlap"]:
                        table.at[idx, "Overlap"] += "; " + warning
                    else:
                        table.at[idx, "Overlap"] = warning
    return table

def find_start_stop(table, temp_directory):
    """
    Scans in silico amplicons and nearby sequences for start and stop codons.
    
    Arguments:
        table - Table of results from SPIDER
        temp_directory - Temporary directory used by SPIDER to search a sequence

    Returns:
        table - Table with appended columns for start_codon, stop_codon, and in-frame
    """
    table['Closest_Start_Codon'] = ""
    table['Closest_Start_Codon_Matches_Amplicon'] = ""
    table['Closest_Stop_Codon'] = ""
    table['Closest_Stop_Codon_Matches_Amplicon'] = ""
    table['Closest_Start_Stop_In_Frame'] = ""
    for idx, row in table.iterrows():
        if not row['Contig'] == "NA":
            # Grab length of the contig to ensure that sliding doesn't exceed the ends
            contigs = SeqIO.index(f"{temp_directory}/reference.fasta", "fasta")
            contig_length = len(contigs[str(row['Contig'])].seq)

            # Extract 100 bp before and after start/end
            start_search = row['Start'] - 100
            
            # Make sure don't go off ends of contig
            if start_search < 1:
                start_search = 1
            start_dist = row['Start'] - start_search
            end_search = row['End'] + 100
            if end_search > contig_length:
                end_search = contig_length

            # Grab sequence
            extracted_seq, length = extract_target_sequence(row['Contig'], start_search, end_search, temp_directory)
            if row['Strand'] == '-':
                extracted_seq = reverse_complement(extracted_seq)
            extracted_seq = str(extracted_seq)

            # Find start codon locations
            start_codon_idx = [codon.start() for codon in re.finditer('ATG', extracted_seq)]
            if len(start_codon_idx) > 0:
                start_codon_idx = np.array(start_codon_idx)
                # Find distances from the start
                start_codon_distances = start_codon_idx - start_dist
                start_codon_distances_abs = abs(start_codon_distances)
                closest_start_codon_idx = start_codon_idx[np.argmin(start_codon_distances_abs)]

                # Add start codon result
                closest_start_codon_position = closest_start_codon_idx - start_dist + row['Start']
                table.at[idx, "Closest_Start_Codon"] = closest_start_codon_position
                table.at[idx, 'Closest_Start_Codon_Matches_Amplicon'] = closest_start_codon_position == row['Start']
            else:
                table.at[idx, "Closest_Start_Codon"] = "No start codons found"

            # Find stop codon locations
            stop_codons = ["TAA", "TAG", "TGA"]
            stop_codon_idx = []
            for stop_codon in stop_codons:
                stop_codon_idx += [codon.start() for codon in re.finditer(stop_codon, extracted_seq)]

            if len(stop_codon_idx) > 0:
                stop_codon_idx = np.array(stop_codon_idx)

                # Find distances from the end
                stop_codon_distances = stop_codon_idx - (row['Target_Length'] - 2 + start_dist)
                stop_codon_distances_abs = abs(stop_codon_distances)
                closest_stop_codon_idx = stop_codon_idx[np.argmin(stop_codon_distances_abs)]

                # Add stop codon result
                closest_stop_codon_position = closest_stop_codon_idx - start_dist + row['Start']
                table.at[idx, "Closest_Stop_Codon"] = closest_stop_codon_position
                table.at[idx, 'Closest_Stop_Codon_Matches_Amplicon'] = closest_stop_codon_position + 2 == row['End']
            else:
                table.at[idx, "Closest_Stop_Codon"] = "No stop codons found"

            # Check that closest start and stop codons are in frame with one another
            if len(start_codon_idx) > 0 and len(stop_codon_idx) > 0:
                closest_in_frame = (closest_stop_codon_idx - closest_start_codon_idx) % 3 == 0
                table.at[idx, "Closest_Start_Stop_In_Frame"] = closest_in_frame
            
    return table

def find_annotations(table, annotation, temp_directory):
    try:
        # Create temporary table
        mod_table = table
        mod_table['Annotation_Match'] = ""
        # Read GFF3
        ann_table = pd.read_csv(annotation, comment="#", sep="\t", header=None, names=GFF3_COLUMNS)
        ann_table = ann_table[ann_table["type"] == "gene"]
        ann_table["append"] = ann_table['start'].astype(str) + "-" + ann_table['end'].astype(str) + ":" + ann_table['attributes'].astype(str)

        # Go through each entry
        for idx, row in mod_table.iterrows():
            if not row['Contig'] == "NA":
                ann_filtered = ann_table[(ann_table['strand'] == row['Strand']) & (row['End'] >= ann_table['start']) & (ann_table['end'] >= row['Start'])]
                if len(ann_filtered) > 0:
                    # Insert all overlapping annotated genes
                    mod_table.at[idx, "Annotation_match"] = ";".join(ann_filtered['append'])
        return mod_table
    except FileNotFoundError:
        print(f"ERROR: The file {annotation} does not exist.", file=sys.stderr)
    except pd.errors.EmptyDataError:
        print(f"ERROR: The file {annotation} is empty.", file=sys.stderr)
    except pd.errors.ParserError:
        print(f"ERROR: The file {annotation} is malformed.", file=sys.stderr)
    except KeyError:
        print(f"ERROR: The file {annotation} is not in the correct format. Make sure your input to --annotation is a valid GFF3 file with 9 columns.", file=sys.stderr)
    except UnicodeDecodeError:
        print(f"ERROR: The file {annotation} is not in the correct format. Make sure your input to --annotation is a valid GFF3 file with 9 columns.", file=sys.stderr)
    except:
        print("ERROR: An error occured while trying to parse annotations. Annotations will not be parsed.", file=sys.stderr)
    return table